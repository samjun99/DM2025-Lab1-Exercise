{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Samuel Montes Jr\n",
    "\n",
    "Student ID: 114065429\n",
    "\n",
    "GitHub ID: samjun99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Code Setup and Data Preparation\n",
    "\n",
    "Similar to the Master exercises, we used the similar code setup for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\samue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\NTHU\\Data_mining\\Labs\\DM2025Labs\\DM2025-Lab1-Exercise\\.venv\\Scripts\\python.exe\n",
      "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# test code for environment setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly as py\n",
    "import math\n",
    "import PAMI\n",
    "import umap\n",
    "\n",
    "# TEST necessary for when working with external scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"newdataset/Reddit-stock-sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the first five records of the dataset to check\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Checking for Missing Values (From Exercise 4)\n",
    "\n",
    "Here, we checked if there's anthing missing values from the data. We can also check the attributes associated to the dataset to see which attributes we can keep for the rest of the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we try to check the total number of missing values per column\n",
    "\n",
    "import helpers.data_mining_helpers as dmh\n",
    "\n",
    "data.isnull().apply(lambda x: dmh.check_missing_values(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In checking the attributes for missing values, it can be seen that both `downvotes` and `upvote_ratio` has majority of its missing values. In my assumption, a valid reason for this could just mean that theres no vote for that particular record whenever we encounter a missing value.\n",
    "\n",
    "In this data, I find that the following attributes: `type`, `subreddit`, `title`, `text`, and `label` is much more meaningful and therefore will be used for the suceeding exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"type\", \"subreddit\", \"title\", \"text\", \"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Category Distribution (From Exercises 7 & 8)\n",
    "\n",
    "In this part, we will try to check the distribution of the dataset. But first, we will need to determine the attribute which will be used for showing the distribution.\n",
    "\n",
    "We will first plot the distribution of the `subreddit` attribute. We added the count labels on the top of the bars for easier checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_counts = data[\"subreddit\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot as bar chart\n",
    "axes = subreddit_counts.plot(kind=\"bar\", title=\"subreddit Distribution\", rot=0)\n",
    "\n",
    "plt.xlabel(\"subreddit\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "for container in axes.containers:\n",
    "    axes.bar_label(container)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try to plot the distribution of other attributes. We have chosen `type` and `labels` in this case since they are the ones that shares the same values on all records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "data['type'].value_counts().plot(kind='bar', ax=axes[0], title=\"type Distribution\", rot=0)\n",
    "axes[0].set_xlabel(\"type\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container)\n",
    "\n",
    "data['label'].value_counts().plot(kind='bar', ax=axes[1], title=\"label Distribution\", rot=0)\n",
    "axes[1].set_xlabel(\"label\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "for container in axes[1].containers:\n",
    "    axes[1].bar_label(container)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 With sampling\n",
    "\n",
    "In this part we will try to do a sampling of the data and check its distribution along with its original data side-by-side. A sampling number of 300 will be used in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = data.sample(n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# First subplot\n",
    "df1 = pd.DataFrame({\n",
    "    \"Main Dataset\": data[\"subreddit\"].value_counts(),\n",
    "    \"Sampled Dataset\": data_sample[\"subreddit\"].value_counts()\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "df1.plot(kind='bar', ax=axes[0], title=\"subreddit Distribution\", rot=0)\n",
    "axes[0].set_xlabel(\"subreddit\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container, padding=3)\n",
    "\n",
    "# Second subplot\n",
    "df2 = pd.DataFrame({\n",
    "    \"Main Dataset\": data[\"type\"].value_counts(),\n",
    "    \"Sampled Dataset\": data_sample[\"type\"].value_counts()\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "df2.plot(kind='bar', ax=axes[1], title=\"type Distribution\", rot=0)\n",
    "axes[1].set_xlabel(\"type\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "for container in axes[1].containers:\n",
    "    axes[1].bar_label(container, padding=3)\n",
    "\n",
    "# Third subplot\n",
    "df3 = pd.DataFrame({\n",
    "    \"Main Dataset\": data[\"label\"].value_counts(),\n",
    "    \"Sampled Dataset\": data_sample[\"label\"].value_counts()\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "df3.plot(kind='bar', ax=axes[2], title=\"label Distribution\", rot=0)\n",
    "axes[2].set_xlabel(\"label\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "\n",
    "for container in axes[2].containers:\n",
    "    axes[2].bar_label(container, padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we can observe that the pattern of the distribution remains the same even with the sampling method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Creation\n",
    "\n",
    "In this section, we will now try to split the text of all the records into an array of tokens and we will try to visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain unigrams for each text and add to the dataset\n",
    "\n",
    "data['unigrams'] = data['text'].apply(lambda x: dmh.tokenize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "data_counts = count_vect.fit_transform(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = count_vect.build_analyzer()\n",
    "analyze(data.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first twenty features only for checking\n",
    "plot_x = [\"term_\"+str(i) for i in count_vect.get_feature_names_out()[0:20]]\n",
    "plot_y = [\"doc_\"+ str(i) for i in list(data.index)[0:20]]\n",
    "plot_z = data_counts[0:20, 0:20].toarray()\n",
    "plot_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will now try to recreate the plot using `seaborn` and in the sucessive parts, we will try to recreate other ways to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_todraw = pd.DataFrame(plot_z, columns = plot_x, index = plot_y)\n",
    "plt.subplots(figsize=(9, 7))\n",
    "ax = sns.heatmap(df_todraw,\n",
    "                 cmap=\"PuRd\",\n",
    "                 vmin=0, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to plot using the new dataset using the style from `exercise 11` in the master file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "term_len = 100\n",
    "doc_len = 100\n",
    "\n",
    "plot_x = [\"term_\"+str(i) for i in count_vect.get_feature_names_out()[0:term_len]]\n",
    "plot_y = [\"doc_\"+ str(i) for i in list(data.index)[0:doc_len]]\n",
    "plot_z = data_counts[0:doc_len, 0:term_len].toarray()\n",
    "\n",
    "df_todraw = pd.DataFrame(plot_z, columns = plot_x, index = plot_y)\n",
    "\n",
    "plt.subplots(figsize=(12, 8))\n",
    "sns.heatmap(df_todraw, cmap=\"viridis\", cbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Other ways to visualize\n",
    "\n",
    "In this section, we will now try to use other ways to visualize the tokenized text to see how they appear frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Word Cloud\n",
    "\n",
    "A word cloud tell us how frequent a term is being used by lokking its font size, the larger the text, the more frequent it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "text = ' '.join(data['text']) \n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud of Text Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Frequency chart\n",
    "\n",
    "The frequency chart shows us the top words being used using bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq = np.array(data_counts.sum(axis=0)).flatten()\n",
    "terms = count_vect.get_feature_names_out()\n",
    "term_df = pd.DataFrame({'term': terms, 'count': term_freq})\n",
    "top_terms = term_df.sort_values(by='count', ascending=False).head(20)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=top_terms, x='term', y='count', palette='viridis')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top 20 Most Frequent Terms\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 PCA Scatter Plot\n",
    "\n",
    "This plot shows the documents projected into two dimensions using **Principal Component Analysis (PCA)**.  \n",
    "Each point represents one document, and the position is determined by patterns in the words used.  \n",
    "\n",
    "- **X-axis (PC1)** shows the direction of greatest variation in the dataset.  \n",
    "- **Y-axis (PC2)** shows the second most significant pattern, uncorrelated with the first.\n",
    "- \n",
    "Documents that appear **close together** have **similar word usage**, while those **far apart** differ more in their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(data_counts.toarray())\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=reduced[:,0], y=reduced[:,1])\n",
    "plt.title(\"PCA Projection of Documents\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transform Text Data\n",
    "\n",
    "In this part, we will extract the word frequency of the `text` field in the dataset. Similar to the one we just showed in the plot of the most frequently used words but with more terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequencies = []\n",
    "for j in range(0,data_counts.shape[1]):\n",
    "    term_frequencies.append(sum(data_counts[:,j].toarray()))\n",
    "\n",
    "term_frequencies = np.asarray(data_counts.sum(axis=0))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier readability, we will only show up to 300 terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# instead of seaborn barplot\n",
    "fig = px.bar(x=count_vect.get_feature_names_out()[:300],\n",
    "             y=term_frequencies[:300],\n",
    "             labels={\"x\": \"Term\", \"y\": \"Frequency\"})\n",
    "\n",
    "# rotate labels for readability\n",
    "fig.update_layout(xaxis_tickangle=-90, height=600, width=1200)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now then sort the terms to see its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_terms = pd.DataFrame({\n",
    "    \"term\": count_vect.get_feature_names_out(),\n",
    "    \"frequency\": term_frequencies\n",
    "})\n",
    "\n",
    "# sort by frequency (descending)\n",
    "df_terms = df_terms.sort_values(\"frequency\", ascending=False)\n",
    "df_terms_sorted = df_terms[:100]\n",
    "\n",
    "# plot sorted bar chart\n",
    "fig = px.bar(df_terms_sorted,\n",
    "             x=\"term\",\n",
    "             y=\"frequency\",\n",
    "             labels={\"term\": \"Term\", \"frequency\": \"Frequency\"})\n",
    "\n",
    "fig.update_layout(xaxis_tickangle=-90, height=600, width=1200)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Transform into log scale\n",
    "\n",
    "We will now transform the data into log scale for more easier readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "term_frequencies_log = [math.log(i) for i in term_frequencies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(100, 30))\n",
    "g = sns.barplot(x=count_vect.get_feature_names_out()[:300],\n",
    "                y=term_frequencies_log[:300])\n",
    "g.set_xticklabels(count_vect.get_feature_names_out()[:300], rotation = 90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will sort them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_terms = pd.DataFrame({\n",
    "    \"term\": count_vect.get_feature_names_out(),\n",
    "    \"frequency\": term_frequencies_log\n",
    "})\n",
    "\n",
    "# sort by frequency (descending)\n",
    "df_terms = df_terms.sort_values(\"frequency\", ascending=False)\n",
    "df_terms_sorted = df_terms[:300]\n",
    "\n",
    "# plot sorted bar chart\n",
    "fig = px.bar(df_terms_sorted,\n",
    "             x=\"term\",\n",
    "             y=\"frequency\",\n",
    "             labels={\"term\": \"Term\", \"frequency\": \"Frequency\"})\n",
    "\n",
    "fig.update_layout(xaxis_tickangle=-90, height=600, width=1200)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
